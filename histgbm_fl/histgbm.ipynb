{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/xichenshe/IAI/scikit-learn'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "os.chdir(\"/Users/xichenshe/IAI/scikit-learn/\")\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABC, abstractmethod\n",
    "from functools import partial\n",
    "import warnings\n",
    "\n",
    "import numpy as np\n",
    "from timeit import default_timer as time\n",
    "\n",
    "from sklearn._loss.loss import (\n",
    "    _LOSSES,\n",
    "    BaseLoss,\n",
    "    AbsoluteError,\n",
    "    HalfBinomialLoss,\n",
    "    HalfMultinomialLoss,\n",
    "    HalfPoissonLoss,\n",
    "    HalfSquaredError,\n",
    "    PinballLoss,\n",
    ")\n",
    "from sklearn.base import BaseEstimator, RegressorMixin, ClassifierMixin, is_classifier\n",
    "from sklearn.utils import check_random_state, resample\n",
    "from sklearn.utils.validation import (\n",
    "    check_is_fitted,\n",
    "    check_consistent_length,\n",
    "    _check_sample_weight,\n",
    ")\n",
    "\n",
    "from sklearn.utils._openmp_helpers import _openmp_effective_n_threads\n",
    "from sklearn.utils.multiclass import check_classification_targets\n",
    "from sklearn.metrics import check_scoring\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "from sklearn.ensemble._hist_gradient_boosting._gradient_boosting import _update_raw_predictions\n",
    "from sklearn.ensemble._hist_gradient_boosting.common import Y_DTYPE, X_DTYPE, G_H_DTYPE\n",
    "\n",
    "from sklearn.ensemble._hist_gradient_boosting.binning import _BinMapper\n",
    "from sklearn.ensemble._hist_gradient_boosting.grower import TreeGrower"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "X, y = make_classification(n_samples=5000, n_features=5, n_informative=4, n_redundant=0, n_repeated=0, n_classes=2, random_state=23)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=23)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "\n",
    "\n",
    "clf = HistGradientBoostingClassifier(max_depth=2, early_stopping=False, random_state=23, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Breakdown of the `fit` method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X_train\n",
    "y = y_train\n",
    "sample_weight = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_start_time = time()\n",
    "acc_find_split_time = 0.0  # time spent finding the best splits\n",
    "acc_apply_split_time = 0.0  # time spent splitting nodes\n",
    "acc_compute_hist_time = 0.0  # time spent computing histograms\n",
    "# time spent predicting X for gradient and hessians update\n",
    "acc_prediction_time = 0.0\n",
    "X, y = clf._validate_data(X, y, dtype=[X_DTYPE], force_all_finite=False)\n",
    "y = clf._encode_y(y)\n",
    "check_consistent_length(X, y)\n",
    "# Do not create unit sample weights by default to later skip some\n",
    "# computation\n",
    "if sample_weight is not None:\n",
    "    sample_weight = _check_sample_weight(sample_weight, X, dtype=np.float64)\n",
    "    # TODO: remove when PDP supports sample weights\n",
    "    clf._fitted_with_sw = True\n",
    "\n",
    "rng = check_random_state(clf.random_state)\n",
    "\n",
    "# When warm starting, we want to re-use the same seed that was used\n",
    "# the first time fit was called (e.g. for subsampling or for the\n",
    "# train/val split).\n",
    "if not (clf.warm_start and clf._is_fitted()):\n",
    "    clf._random_seed = rng.randint(np.iinfo(np.uint32).max, dtype=\"u8\")\n",
    "\n",
    "clf._validate_parameters()\n",
    "\n",
    "# used for validation in predict\n",
    "n_samples, clf._n_features = X.shape\n",
    "\n",
    "clf.is_categorical_, known_categories = clf._check_categories(X)\n",
    "\n",
    "# we need this stateful variable to tell raw_predict() that it was\n",
    "# called from fit() (this current method), and that the data it has\n",
    "# received is pre-binned.\n",
    "# predicting is faster on pre-binned data, so we want early stopping\n",
    "# predictions to be made on pre-binned data. Unfortunately the _scorer\n",
    "# can only call predict() or predict_proba(), not raw_predict(), and\n",
    "# there's no way to tell the scorer that it needs to predict binned\n",
    "# data.\n",
    "clf._in_fit = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# `_openmp_effective_n_threads` is used to take cgroups CPU quotes\n",
    "# into account when determine the maximum number of threads to use.\n",
    "n_threads = _openmp_effective_n_threads()\n",
    "\n",
    "if isinstance(clf.loss, str):\n",
    "    clf._loss = clf._get_loss(sample_weight=sample_weight)\n",
    "elif isinstance(clf.loss, BaseLoss):\n",
    "    clf._loss = clf.loss\n",
    "\n",
    "if clf.early_stopping == \"auto\":\n",
    "    clf.do_early_stopping_ = n_samples > 10000\n",
    "else:\n",
    "    clf.do_early_stopping_ = clf.early_stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create validation data if needed\n",
    "clf._use_validation_data = clf.validation_fraction is not None\n",
    "if clf.do_early_stopping_ and clf._use_validation_data:\n",
    "    # stratify for classification\n",
    "    # instead of checking predict_proba, loss.n_classes >= 2 would also work\n",
    "    stratify = y if hasattr(clf._loss, \"predict_proba\") else None\n",
    "\n",
    "    # Save the state of the RNG for the training and validation split.\n",
    "    # This is needed in order to have the same split when using\n",
    "    # warm starting.\n",
    "\n",
    "    if sample_weight is None:\n",
    "        X_train, X_val, y_train, y_val = train_test_split(\n",
    "            X,\n",
    "            y,\n",
    "            test_size=clf.validation_fraction,\n",
    "            stratify=stratify,\n",
    "            random_state=clf._random_seed,\n",
    "        )\n",
    "        sample_weight_train = sample_weight_val = None\n",
    "    else:\n",
    "        # TODO: incorporate sample_weight in sampling here, as well as\n",
    "        # stratify\n",
    "        (\n",
    "            X_train,\n",
    "            X_val,\n",
    "            y_train,\n",
    "            y_val,\n",
    "            sample_weight_train,\n",
    "            sample_weight_val,\n",
    "        ) = train_test_split(\n",
    "            X,\n",
    "            y,\n",
    "            sample_weight,\n",
    "            test_size=clf.validation_fraction,\n",
    "            stratify=stratify,\n",
    "            random_state=clf._random_seed,\n",
    "        )\n",
    "else:\n",
    "    X_train, y_train, sample_weight_train = X, y, sample_weight\n",
    "    X_val = y_val = sample_weight_val = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_bins = clf.max_bins + 1  # + 1 for missing values\n",
    "clf._bin_mapper = _BinMapper(\n",
    "    n_bins=n_bins,\n",
    "    is_categorical=clf.is_categorical_,\n",
    "    known_categories=known_categories,\n",
    "    random_state=clf._random_seed,\n",
    "    n_threads=n_threads,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Binning 0.000 GB of training data: 0.007 s\n",
      "Fitting gradient boosted rounds:\n"
     ]
    }
   ],
   "source": [
    "X_binned_train = clf._bin_data(X_train, is_training_data=True, bin_thresholds=None)\n",
    "if X_val is not None:\n",
    "    X_binned_val = clf._bin_data(X_val, is_training_data=False)\n",
    "else:\n",
    "    X_binned_val = None\n",
    "\n",
    "# Uses binned data to check for missing values\n",
    "has_missing_values = (\n",
    "    (X_binned_train == clf._bin_mapper.missing_values_bin_idx_)\n",
    "    .any(axis=0)\n",
    "    .astype(np.uint8)\n",
    ")\n",
    "\n",
    "if clf.verbose:\n",
    "    print(\"Fitting gradient boosted rounds:\")\n",
    "\n",
    "n_samples = X_binned_train.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-2.54182129, -2.36895312, -2.22537003, -2.13057502, -2.05239781,\n",
       "       -1.97522141, -1.90052693, -1.84954353, -1.7951716 , -1.74282237])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf._bin_mapper.bin_thresholds_[0][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-4.03672558, -3.72347704, -3.5660336 , -3.4256315 , -3.31027386,\n",
       "       -3.23354503, -3.14732805, -3.06911889, -3.00062454, -2.93068974])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf._bin_mapper.bin_thresholds_[3][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 67,  44, 101, 149, 110],\n",
       "       [ 22,  98,  26, 169,  11],\n",
       "       [249, 138, 183,  28, 199],\n",
       "       ...,\n",
       "       [252,  88,  85,  87, 246],\n",
       "       [207, 211, 133, 111, 113],\n",
       "       [207,  68, 239, 208, 193]], dtype=uint8)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_binned_train[:2000,]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear random state and score attributes\n",
    "clf._clear_state()\n",
    "\n",
    "# initialize raw_predictions: those are the accumulated values\n",
    "# predicted by the trees for the training data. raw_predictions has\n",
    "# shape (n_samples, n_trees_per_iteration) where\n",
    "# n_trees_per_iterations is n_classes in multiclass classification,\n",
    "# else 1.\n",
    "# clf._baseline_prediction has shape (1, n_trees_per_iteration)\n",
    "clf._baseline_prediction = clf._loss.fit_intercept_only(\n",
    "    y_true=y_train, sample_weight=sample_weight_train\n",
    ").reshape((1, -1))\n",
    "raw_predictions = np.zeros(\n",
    "    shape=(n_samples, clf.n_trees_per_iteration_),\n",
    "    dtype=clf._baseline_prediction.dtype,\n",
    "    order=\"F\",\n",
    ")\n",
    "raw_predictions += clf._baseline_prediction\n",
    "\n",
    "# predictors is a matrix (list of lists) of TreePredictor objects\n",
    "# with shape (n_iter_, n_trees_per_iteration)\n",
    "clf._predictors = predictors = []\n",
    "\n",
    "# Initialize structures and attributes related to early stopping\n",
    "clf._scorer = None  # set if scoring != loss\n",
    "raw_predictions_val = None  # set if scoring == loss and use val\n",
    "clf.train_score_ = []\n",
    "clf.validation_score_ = []\n",
    "\n",
    "if clf.do_early_stopping_:\n",
    "    # populate train_score and validation_score with the\n",
    "    # predictions of the initial model (before the first tree)\n",
    "\n",
    "    if clf.scoring == \"loss\":\n",
    "        # we're going to compute scoring w.r.t the loss. As losses\n",
    "        # take raw predictions as input (unlike the scorers), we\n",
    "        # can optimize a bit and avoid repeating computing the\n",
    "        # predictions of the previous trees. We'll re-use\n",
    "        # raw_predictions (as it's needed for training anyway) for\n",
    "        # evaluating the training loss, and create\n",
    "        # raw_predictions_val for storing the raw predictions of\n",
    "        # the validation data.\n",
    "\n",
    "        if clf._use_validation_data:\n",
    "            raw_predictions_val = np.zeros(\n",
    "                shape=(X_binned_val.shape[0], clf.n_trees_per_iteration_),\n",
    "                dtype=clf._baseline_prediction.dtype,\n",
    "                order=\"F\",\n",
    "            )\n",
    "\n",
    "            raw_predictions_val += clf._baseline_prediction\n",
    "\n",
    "        clf._check_early_stopping_loss(\n",
    "            raw_predictions=raw_predictions,\n",
    "            y_train=y_train,\n",
    "            sample_weight_train=sample_weight_train,\n",
    "            raw_predictions_val=raw_predictions_val,\n",
    "            y_val=y_val,\n",
    "            sample_weight_val=sample_weight_val,\n",
    "            n_threads=n_threads,\n",
    "        )\n",
    "    else:\n",
    "        clf._scorer = check_scoring(clf, clf.scoring)\n",
    "        # _scorer is a callable with signature (est, X, y) and\n",
    "        # calls est.predict() or est.predict_proba() depending on\n",
    "        # its nature.\n",
    "        # Unfortunately, each call to _scorer() will compute\n",
    "        # the predictions of all the trees. So we use a subset of\n",
    "        # the training set to compute train scores.\n",
    "\n",
    "        # Compute the subsample set\n",
    "        (\n",
    "            X_binned_small_train,\n",
    "            y_small_train,\n",
    "            sample_weight_small_train,\n",
    "        ) = clf._get_small_trainset(\n",
    "            X_binned_train, y_train, sample_weight_train, clf._random_seed\n",
    "        )\n",
    "\n",
    "        clf._check_early_stopping_scorer(\n",
    "            X_binned_small_train,\n",
    "            y_small_train,\n",
    "            sample_weight_small_train,\n",
    "            X_binned_val,\n",
    "            y_val,\n",
    "            sample_weight_val,\n",
    "        )\n",
    "begin_at_stage = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.01200014],\n",
       "       [-0.01200014],\n",
       "       [-0.01200014],\n",
       "       ...,\n",
       "       [-0.01200014],\n",
       "       [-0.01200014],\n",
       "       [-0.01200014]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize gradients and hessians (empty arrays).\n",
    "# shape = (n_samples, n_trees_per_iteration).\n",
    "gradient, hessian = clf._loss.init_gradient_and_hessian(\n",
    "    n_samples=n_samples, dtype=G_H_DTYPE, order=\"F\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update gradients and hessians, inplace\n",
    "# Note that clf._loss expects shape (n_samples,) for\n",
    "# n_trees_per_iteration = 1 else shape (n_samples, n_trees_per_iteration).\n",
    "if clf._loss.constant_hessian:\n",
    "    clf._loss.gradient(\n",
    "        y_true=y_train,\n",
    "        raw_prediction=raw_predictions,\n",
    "        sample_weight=sample_weight_train,\n",
    "        gradient_out=gradient,\n",
    "        n_threads=n_threads,\n",
    "    )\n",
    "else:\n",
    "    clf._loss.gradient_hessian(\n",
    "        y_true=y_train,\n",
    "        raw_prediction=raw_predictions,\n",
    "        sample_weight=sample_weight_train,\n",
    "        gradient_out=gradient,\n",
    "        hessian_out=hessian,\n",
    "        n_threads=n_threads,\n",
    "    )\n",
    "\n",
    "# Append a list since there may be more than 1 predictor per iter\n",
    "predictors.append([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2-d views of shape (n_samples, n_trees_per_iteration_) or (n_samples, 1)\n",
    "# on gradient and hessian to simplify the loop over n_trees_per_iteration_.\n",
    "if gradient.ndim == 1:\n",
    "    g_view = gradient.reshape((-1, 1))\n",
    "    h_view = hessian.reshape((-1, 1))\n",
    "else:\n",
    "    g_view = gradient\n",
    "    h_view = hessian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.497],\n",
       "       [ 0.497],\n",
       "       [-0.503],\n",
       "       [ 0.497],\n",
       "       [-0.503],\n",
       "       [-0.503],\n",
       "       [ 0.497],\n",
       "       [ 0.497],\n",
       "       [ 0.497],\n",
       "       [-0.503]], dtype=float32)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g_view[:10,]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.249991],\n",
       "       [0.249991],\n",
       "       [0.249991],\n",
       "       [0.249991],\n",
       "       [0.249991],\n",
       "       [0.249991],\n",
       "       [0.249991],\n",
       "       [0.249991],\n",
       "       [0.249991],\n",
       "       [0.249991]], dtype=float32)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h_view[:10,]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.503],\n",
       "       [ 0.497],\n",
       "       [ 0.497],\n",
       "       [-0.503],\n",
       "       [ 0.497],\n",
       "       [-0.503],\n",
       "       [-0.503],\n",
       "       [ 0.497],\n",
       "       [-0.503],\n",
       "       [ 0.497]], dtype=float32)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g_view[-10:,]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 0\n",
    "\n",
    "grower = TreeGrower(\n",
    "    X_binned=X_binned_train,\n",
    "    gradients=g_view[:, k],\n",
    "    hessians=h_view[:, k],\n",
    "    n_bins=n_bins,\n",
    "    n_bins_non_missing=clf._bin_mapper.n_bins_non_missing_,\n",
    "    has_missing_values=has_missing_values,\n",
    "    is_categorical=clf.is_categorical_,\n",
    "    monotonic_cst=clf.monotonic_cst,\n",
    "    max_leaf_nodes=clf.max_leaf_nodes,\n",
    "    max_depth=clf.max_depth,\n",
    "    min_samples_leaf=clf.min_samples_leaf,\n",
    "    l2_regularization=clf.l2_regularization,\n",
    "    shrinkage=clf.learning_rate,\n",
    "    n_threads=n_threads,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([(-0.0480001 , 3.999856, 16), (-0.0480001 , 3.999856, 16),\n",
       "       ( 1.95199996, 3.999856, 16), (-1.54500014, 3.749865, 15),\n",
       "       (-1.04800013, 3.999856, 16), ( 0.95199993, 3.999856, 16),\n",
       "       ( 3.45500001, 3.749865, 15), (-2.04800016, 3.999856, 16),\n",
       "       ( 1.95199996, 3.999856, 16), (-3.5450002 , 3.749865, 15)],\n",
       "      dtype=[('sum_gradients', '<f8'), ('sum_hessians', '<f8'), ('count', '<u4')])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.asarray(grower.root.histograms)[0][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'depth': 0,\n",
       " 'sample_indices': <MemoryView of 'ndarray' at 0x1265e66c0>,\n",
       " 'n_samples': 4000,\n",
       " 'sum_gradients': -2.396106719970703e-05,\n",
       " 'sum_hessians': 999.9639987945557,\n",
       " 'value': 0,\n",
       " 'is_leaf': False,\n",
       " 'children_lower_bound': -inf,\n",
       " 'children_upper_bound': inf,\n",
       " 'partition_start': 0,\n",
       " 'partition_stop': 4000,\n",
       " 'histograms': <MemoryView of 'ndarray' at 0x1265e6ba0>,\n",
       " 'split_info': <sklearn.ensemble._hist_gradient_boosting.splitting.SplitInfo at 0x126619490>}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grower.root.__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'gain': 764.4530248414894,\n",
       " 'feature_idx': 4,\n",
       " 'bin_idx': 195,\n",
       " 'missing_go_to_left': 0,\n",
       " 'sum_gradient_left': 368.7779925763607,\n",
       " 'sum_hessian_left': 768.472333073616,\n",
       " 'sum_gradient_right': -368.7780165374279,\n",
       " 'sum_hessian_right': 231.49166572093964,\n",
       " 'n_samples_left': 3074,\n",
       " 'n_samples_right': 926,\n",
       " 'value_left': -0.479884540672245,\n",
       " 'value_right': 1.59305094370864,\n",
       " 'is_categorical': 0,\n",
       " 'left_cat_bitset': None}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grower.root.split_info.__dict__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build child nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from heapq import heappush, heappop\n",
    "import numpy as np\n",
    "from timeit import default_timer as time\n",
    "import numbers\n",
    "\n",
    "\n",
    "node = heappop(grower.splittable_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    sample_indices_left,\n",
    "    sample_indices_right,\n",
    "    right_child_pos,\n",
    ") = grower.splitter.split_indices(node.split_info, node.sample_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "depth = node.depth + 1\n",
    "n_leaf_nodes = len(grower.finalized_leaves) + len(grower.splittable_nodes)\n",
    "n_leaf_nodes += 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble._hist_gradient_boosting.grower import TreeNode\n",
    "\n",
    "\n",
    "left_child_node = TreeNode(\n",
    "    depth,\n",
    "    sample_indices_left,\n",
    "    node.split_info.sum_gradient_left,\n",
    "    node.split_info.sum_hessian_left,\n",
    "    value=node.split_info.value_left,\n",
    ")\n",
    "right_child_node = TreeNode(\n",
    "    depth,\n",
    "    sample_indices_right,\n",
    "    node.split_info.sum_gradient_right,\n",
    "    node.split_info.sum_hessian_right,\n",
    "    value=node.split_info.value_right,\n",
    ")\n",
    "\n",
    "node.right_child = right_child_node\n",
    "node.left_child = left_child_node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set start and stop indices\n",
    "left_child_node.partition_start = node.partition_start\n",
    "left_child_node.partition_stop = node.partition_start + right_child_pos\n",
    "right_child_node.partition_start = left_child_node.partition_stop\n",
    "right_child_node.partition_stop = node.partition_stop\n",
    "\n",
    "if not grower.has_missing_values[node.split_info.feature_idx]:\n",
    "    # If no missing values are encountered at fit time, then samples\n",
    "    # with missing values during predict() will go to whichever child\n",
    "    # has the most samples.\n",
    "    node.split_info.missing_go_to_left = (\n",
    "        left_child_node.n_samples > right_child_node.n_samples\n",
    "    )\n",
    "\n",
    "grower.n_nodes += 2\n",
    "grower.n_categorical_splits += node.split_info.is_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grower.max_depth is not None and depth == grower.max_depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if grower.max_leaf_nodes is not None and n_leaf_nodes == grower.max_leaf_nodes:\n",
    "#     grower._finalize_leaf(left_child_node)\n",
    "#     grower._finalize_leaf(right_child_node)\n",
    "#     grower._finalize_splittable_nodes()\n",
    "#     # return left_child_node, right_child_node\n",
    "\n",
    "if grower.max_depth is not None and depth == grower.max_depth:\n",
    "    grower._finalize_leaf(left_child_node)\n",
    "    grower._finalize_leaf(right_child_node)\n",
    "    # return left_child_node, right_child_node\n",
    "else:\n",
    "    # We will compute the histograms of both nodes even if one of them\n",
    "    # is a leaf, since computing the second histogram is very cheap\n",
    "    # (using histogram subtraction).\n",
    "    n_samples_left = left_child_node.sample_indices.shape[0]\n",
    "    n_samples_right = right_child_node.sample_indices.shape[0]\n",
    "    if n_samples_left < n_samples_right:\n",
    "        smallest_child = left_child_node\n",
    "        largest_child = right_child_node\n",
    "    else:\n",
    "        smallest_child = right_child_node\n",
    "        largest_child = left_child_node\n",
    "\n",
    "    smallest_child.histograms = grower.histogram_builder.compute_histograms_brute(\n",
    "        smallest_child.sample_indices\n",
    "    )\n",
    "    largest_child.histograms = (\n",
    "        grower.histogram_builder.compute_histograms_subtraction(\n",
    "            node.histograms, smallest_child.histograms\n",
    "        )\n",
    "    )\n",
    "    grower._compute_best_split_and_push(left_child_node)\n",
    "    grower._compute_best_split_and_push(right_child_node)\n",
    "\n",
    "    # # For LEAF nodes, release memory used by histograms as they are no longer needed\n",
    "    # # since they won't be split.\n",
    "    # for child in (left_child_node, right_child_node):\n",
    "    #     if child.is_leaf:\n",
    "    #         del child.histograms\n",
    "    \n",
    "    # # For INTERNAL nodes, release memory used by histograms as they are no longer needed\n",
    "    # # once children histograms have been computed.\n",
    "    # del node.histograms\n",
    "\n",
    "# if left_child_node.n_samples < grower.min_samples_leaf * 2:\n",
    "#     grower._finalize_leaf(left_child_node)\n",
    "# if right_child_node.n_samples < grower.min_samples_leaf * 2:\n",
    "#     grower._finalize_leaf(right_child_node)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grower.splittable_nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'gain': 775.8972238270022,\n",
       " 'feature_idx': 1,\n",
       " 'bin_idx': 165,\n",
       " 'missing_go_to_left': True,\n",
       " 'sum_gradient_left': 608.0060061514378,\n",
       " 'sum_hessian_left': 499.48201739788055,\n",
       " 'sum_gradient_right': -239.22801357507706,\n",
       " 'sum_hessian_right': 268.9903156757355,\n",
       " 'n_samples_left': 1998,\n",
       " 'n_samples_right': 1076,\n",
       " 'value_left': -1.2172730648421093,\n",
       " 'value_right': 0.8893554884089712,\n",
       " 'is_categorical': 0,\n",
       " 'left_cat_bitset': None}"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grower.root.left_child.split_info.__dict__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make predictors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>value</th>\n",
       "      <th>count</th>\n",
       "      <th>feature_idx</th>\n",
       "      <th>num_threshold</th>\n",
       "      <th>missing_go_to_left</th>\n",
       "      <th>left</th>\n",
       "      <th>right</th>\n",
       "      <th>gain</th>\n",
       "      <th>depth</th>\n",
       "      <th>is_leaf</th>\n",
       "      <th>bin_threshold</th>\n",
       "      <th>is_categorical</th>\n",
       "      <th>bitset_idx</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>4000</td>\n",
       "      <td>4</td>\n",
       "      <td>0.394379</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>764.453025</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>195</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.479885</td>\n",
       "      <td>3074</td>\n",
       "      <td>1</td>\n",
       "      <td>0.154310</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>775.897224</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>165</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-1.217273</td>\n",
       "      <td>1998</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.889355</td>\n",
       "      <td>1076</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.593051</td>\n",
       "      <td>926</td>\n",
       "      <td>3</td>\n",
       "      <td>1.029580</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>43.621109</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>216</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1.720982</td>\n",
       "      <td>852</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.120112</td>\n",
       "      <td>74</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      value  count  feature_idx  num_threshold  missing_go_to_left  left  \\\n",
       "0  0.000000   4000            4       0.394379                   1     1   \n",
       "1 -0.479885   3074            1       0.154310                   1     2   \n",
       "2 -1.217273   1998            0       0.000000                   0     0   \n",
       "3  0.889355   1076            0       0.000000                   0     0   \n",
       "4  1.593051    926            3       1.029580                   1     5   \n",
       "5  1.720982    852            0       0.000000                   0     0   \n",
       "6  0.120112     74            0       0.000000                   0     0   \n",
       "\n",
       "   right        gain  depth  is_leaf  bin_threshold  is_categorical  \\\n",
       "0      4  764.453025      0        0            195               0   \n",
       "1      3  775.897224      1        0            165               0   \n",
       "2      0   -1.000000      2        1              0               0   \n",
       "3      0   -1.000000      2        1              0               0   \n",
       "4      6   43.621109      1        0            216               0   \n",
       "5      0   -1.000000      2        1              0               0   \n",
       "6      0   -1.000000      2        1              0               0   \n",
       "\n",
       "   bitset_idx  \n",
       "0           0  \n",
       "1           0  \n",
       "2           0  \n",
       "3           0  \n",
       "4           0  \n",
       "5           0  \n",
       "6           0  "
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "pd.DataFrame(grower.make_predictor(clf._bin_mapper.bin_thresholds_).nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.59138517, -1.34886332,  1.9947005 , ...,  0.82364599,\n",
       "        1.88878201,  1.09721381])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([1.000e+00, 1.500e+01, 1.350e+02, 4.580e+02, 9.790e+02, 1.196e+03,\n",
       "        8.050e+02, 3.340e+02, 6.600e+01, 1.100e+01]),\n",
       " array([-4.05482491, -3.29252876, -2.53023261, -1.76793646, -1.00564031,\n",
       "        -0.24334416,  0.51895199,  1.28124814,  2.04354429,  2.80584044,\n",
       "         3.56813659]),\n",
       " <BarContainer object of 10 artists>)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(X_train[:, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2237"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(X_train[:, 0] <= 0.154)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1942"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(X_train[:, 0] <= -0.027)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "da3de42dcda9fd7691867a6931ab9341a8446acb0162577900f5e63b41b9f416"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 ('scikit-learn-bjglRoMY')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
