{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/xichenshe/IAI/scikit-learn'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "os.chdir(\"/Users/xichenshe/IAI/scikit-learn/\")\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABC, abstractmethod\n",
    "from functools import partial\n",
    "import warnings\n",
    "\n",
    "import numpy as np\n",
    "from timeit import default_timer as time\n",
    "\n",
    "from sklearn._loss.loss import (\n",
    "    _LOSSES,\n",
    "    BaseLoss,\n",
    "    AbsoluteError,\n",
    "    HalfBinomialLoss,\n",
    "    HalfMultinomialLoss,\n",
    "    HalfPoissonLoss,\n",
    "    HalfSquaredError,\n",
    "    PinballLoss,\n",
    ")\n",
    "from sklearn.base import BaseEstimator, RegressorMixin, ClassifierMixin, is_classifier\n",
    "from sklearn.utils import check_random_state, resample\n",
    "from sklearn.utils.validation import (\n",
    "    check_is_fitted,\n",
    "    check_consistent_length,\n",
    "    _check_sample_weight,\n",
    ")\n",
    "\n",
    "from sklearn.utils._openmp_helpers import _openmp_effective_n_threads\n",
    "from sklearn.utils.multiclass import check_classification_targets\n",
    "from sklearn.metrics import check_scoring\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "from sklearn.ensemble._hist_gradient_boosting._gradient_boosting import _update_raw_predictions\n",
    "from sklearn.ensemble._hist_gradient_boosting.common import Y_DTYPE, X_DTYPE, G_H_DTYPE\n",
    "\n",
    "from sklearn.ensemble._hist_gradient_boosting.binning import _BinMapper\n",
    "from sklearn.ensemble._hist_gradient_boosting.grower import TreeGrower"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "X, y = make_classification(n_samples=5000, n_features=5, n_informative=4, n_redundant=0, n_repeated=0, n_classes=2, random_state=23)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=23)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "\n",
    "\n",
    "clf = HistGradientBoostingClassifier(max_depth=2, early_stopping=False, random_state=23, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Breakdown of the `fit` method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X_train\n",
    "y = y_train\n",
    "sample_weight = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_start_time = time()\n",
    "acc_find_split_time = 0.0  # time spent finding the best splits\n",
    "acc_apply_split_time = 0.0  # time spent splitting nodes\n",
    "acc_compute_hist_time = 0.0  # time spent computing histograms\n",
    "# time spent predicting X for gradient and hessians update\n",
    "acc_prediction_time = 0.0\n",
    "X, y = clf._validate_data(X, y, dtype=[X_DTYPE], force_all_finite=False)\n",
    "y = clf._encode_y(y)\n",
    "check_consistent_length(X, y)\n",
    "# Do not create unit sample weights by default to later skip some\n",
    "# computation\n",
    "if sample_weight is not None:\n",
    "    sample_weight = _check_sample_weight(sample_weight, X, dtype=np.float64)\n",
    "    # TODO: remove when PDP supports sample weights\n",
    "    clf._fitted_with_sw = True\n",
    "\n",
    "rng = check_random_state(clf.random_state)\n",
    "\n",
    "# When warm starting, we want to re-use the same seed that was used\n",
    "# the first time fit was called (e.g. for subsampling or for the\n",
    "# train/val split).\n",
    "if not (clf.warm_start and clf._is_fitted()):\n",
    "    clf._random_seed = rng.randint(np.iinfo(np.uint32).max, dtype=\"u8\")\n",
    "\n",
    "clf._validate_parameters()\n",
    "\n",
    "# used for validation in predict\n",
    "n_samples, clf._n_features = X.shape\n",
    "\n",
    "clf.is_categorical_, known_categories = clf._check_categories(X)\n",
    "\n",
    "# we need this stateful variable to tell raw_predict() that it was\n",
    "# called from fit() (this current method), and that the data it has\n",
    "# received is pre-binned.\n",
    "# predicting is faster on pre-binned data, so we want early stopping\n",
    "# predictions to be made on pre-binned data. Unfortunately the _scorer\n",
    "# can only call predict() or predict_proba(), not raw_predict(), and\n",
    "# there's no way to tell the scorer that it needs to predict binned\n",
    "# data.\n",
    "clf._in_fit = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# `_openmp_effective_n_threads` is used to take cgroups CPU quotes\n",
    "# into account when determine the maximum number of threads to use.\n",
    "n_threads = _openmp_effective_n_threads()\n",
    "\n",
    "if isinstance(clf.loss, str):\n",
    "    clf._loss = clf._get_loss(sample_weight=sample_weight)\n",
    "elif isinstance(clf.loss, BaseLoss):\n",
    "    clf._loss = clf.loss\n",
    "\n",
    "if clf.early_stopping == \"auto\":\n",
    "    clf.do_early_stopping_ = n_samples > 10000\n",
    "else:\n",
    "    clf.do_early_stopping_ = clf.early_stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create validation data if needed\n",
    "clf._use_validation_data = clf.validation_fraction is not None\n",
    "if clf.do_early_stopping_ and clf._use_validation_data:\n",
    "    # stratify for classification\n",
    "    # instead of checking predict_proba, loss.n_classes >= 2 would also work\n",
    "    stratify = y if hasattr(clf._loss, \"predict_proba\") else None\n",
    "\n",
    "    # Save the state of the RNG for the training and validation split.\n",
    "    # This is needed in order to have the same split when using\n",
    "    # warm starting.\n",
    "\n",
    "    if sample_weight is None:\n",
    "        X_train, X_val, y_train, y_val = train_test_split(\n",
    "            X,\n",
    "            y,\n",
    "            test_size=clf.validation_fraction,\n",
    "            stratify=stratify,\n",
    "            random_state=clf._random_seed,\n",
    "        )\n",
    "        sample_weight_train = sample_weight_val = None\n",
    "    else:\n",
    "        # TODO: incorporate sample_weight in sampling here, as well as\n",
    "        # stratify\n",
    "        (\n",
    "            X_train,\n",
    "            X_val,\n",
    "            y_train,\n",
    "            y_val,\n",
    "            sample_weight_train,\n",
    "            sample_weight_val,\n",
    "        ) = train_test_split(\n",
    "            X,\n",
    "            y,\n",
    "            sample_weight,\n",
    "            test_size=clf.validation_fraction,\n",
    "            stratify=stratify,\n",
    "            random_state=clf._random_seed,\n",
    "        )\n",
    "else:\n",
    "    X_train, y_train, sample_weight_train = X, y, sample_weight\n",
    "    X_val = y_val = sample_weight_val = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_bins = clf.max_bins + 1  # + 1 for missing values\n",
    "clf._bin_mapper = _BinMapper(\n",
    "    n_bins=n_bins,\n",
    "    is_categorical=clf.is_categorical_,\n",
    "    known_categories=known_categories,\n",
    "    random_state=clf._random_seed,\n",
    "    n_threads=n_threads,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Binning 0.000 GB of training data: 0.011 s\n",
      "Fitting gradient boosted rounds:\n"
     ]
    }
   ],
   "source": [
    "X_binned_train = clf._bin_data(X_train, is_training_data=True, bin_thresholds=None)\n",
    "if X_val is not None:\n",
    "    X_binned_val = clf._bin_data(X_val, is_training_data=False)\n",
    "else:\n",
    "    X_binned_val = None\n",
    "\n",
    "# Uses binned data to check for missing values\n",
    "has_missing_values = (\n",
    "    (X_binned_train == clf._bin_mapper.missing_values_bin_idx_)\n",
    "    .any(axis=0)\n",
    "    .astype(np.uint8)\n",
    ")\n",
    "\n",
    "if clf.verbose:\n",
    "    print(\"Fitting gradient boosted rounds:\")\n",
    "\n",
    "n_samples = X_binned_train.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-2.54182129, -2.36895312, -2.22537003, -2.13057502, -2.05239781,\n",
       "       -1.97522141, -1.90052693, -1.84954353, -1.7951716 , -1.74282237])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf._bin_mapper.bin_thresholds_[0][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-4.03672558, -3.72347704, -3.5660336 , -3.4256315 , -3.31027386,\n",
       "       -3.23354503, -3.14732805, -3.06911889, -3.00062454, -2.93068974])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf._bin_mapper.bin_thresholds_[3][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 67,  44, 101, 149, 110],\n",
       "       [ 22,  98,  26, 169,  11],\n",
       "       [249, 138, 183,  28, 199],\n",
       "       ...,\n",
       "       [252,  88,  85,  87, 246],\n",
       "       [207, 211, 133, 111, 113],\n",
       "       [207,  68, 239, 208, 193]], dtype=uint8)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_binned_train[:2000,]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear random state and score attributes\n",
    "clf._clear_state()\n",
    "\n",
    "# initialize raw_predictions: those are the accumulated values\n",
    "# predicted by the trees for the training data. raw_predictions has\n",
    "# shape (n_samples, n_trees_per_iteration) where\n",
    "# n_trees_per_iterations is n_classes in multiclass classification,\n",
    "# else 1.\n",
    "# clf._baseline_prediction has shape (1, n_trees_per_iteration)\n",
    "clf._baseline_prediction = clf._loss.fit_intercept_only(\n",
    "    y_true=y_train, sample_weight=sample_weight_train\n",
    ").reshape((1, -1))\n",
    "raw_predictions = np.zeros(\n",
    "    shape=(n_samples, clf.n_trees_per_iteration_),\n",
    "    dtype=clf._baseline_prediction.dtype,\n",
    "    order=\"F\",\n",
    ")\n",
    "raw_predictions += clf._baseline_prediction\n",
    "\n",
    "# predictors is a matrix (list of lists) of TreePredictor objects\n",
    "# with shape (n_iter_, n_trees_per_iteration)\n",
    "clf._predictors = predictors = []\n",
    "\n",
    "# Initialize structures and attributes related to early stopping\n",
    "clf._scorer = None  # set if scoring != loss\n",
    "raw_predictions_val = None  # set if scoring == loss and use val\n",
    "clf.train_score_ = []\n",
    "clf.validation_score_ = []\n",
    "\n",
    "if clf.do_early_stopping_:\n",
    "    # populate train_score and validation_score with the\n",
    "    # predictions of the initial model (before the first tree)\n",
    "\n",
    "    if clf.scoring == \"loss\":\n",
    "        # we're going to compute scoring w.r.t the loss. As losses\n",
    "        # take raw predictions as input (unlike the scorers), we\n",
    "        # can optimize a bit and avoid repeating computing the\n",
    "        # predictions of the previous trees. We'll re-use\n",
    "        # raw_predictions (as it's needed for training anyway) for\n",
    "        # evaluating the training loss, and create\n",
    "        # raw_predictions_val for storing the raw predictions of\n",
    "        # the validation data.\n",
    "\n",
    "        if clf._use_validation_data:\n",
    "            raw_predictions_val = np.zeros(\n",
    "                shape=(X_binned_val.shape[0], clf.n_trees_per_iteration_),\n",
    "                dtype=clf._baseline_prediction.dtype,\n",
    "                order=\"F\",\n",
    "            )\n",
    "\n",
    "            raw_predictions_val += clf._baseline_prediction\n",
    "\n",
    "        clf._check_early_stopping_loss(\n",
    "            raw_predictions=raw_predictions,\n",
    "            y_train=y_train,\n",
    "            sample_weight_train=sample_weight_train,\n",
    "            raw_predictions_val=raw_predictions_val,\n",
    "            y_val=y_val,\n",
    "            sample_weight_val=sample_weight_val,\n",
    "            n_threads=n_threads,\n",
    "        )\n",
    "    else:\n",
    "        clf._scorer = check_scoring(clf, clf.scoring)\n",
    "        # _scorer is a callable with signature (est, X, y) and\n",
    "        # calls est.predict() or est.predict_proba() depending on\n",
    "        # its nature.\n",
    "        # Unfortunately, each call to _scorer() will compute\n",
    "        # the predictions of all the trees. So we use a subset of\n",
    "        # the training set to compute train scores.\n",
    "\n",
    "        # Compute the subsample set\n",
    "        (\n",
    "            X_binned_small_train,\n",
    "            y_small_train,\n",
    "            sample_weight_small_train,\n",
    "        ) = clf._get_small_trainset(\n",
    "            X_binned_train, y_train, sample_weight_train, clf._random_seed\n",
    "        )\n",
    "\n",
    "        clf._check_early_stopping_scorer(\n",
    "            X_binned_small_train,\n",
    "            y_small_train,\n",
    "            sample_weight_small_train,\n",
    "            X_binned_val,\n",
    "            y_val,\n",
    "            sample_weight_val,\n",
    "        )\n",
    "begin_at_stage = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.01200014],\n",
       "       [-0.01200014],\n",
       "       [-0.01200014],\n",
       "       ...,\n",
       "       [-0.01200014],\n",
       "       [-0.01200014],\n",
       "       [-0.01200014]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize gradients and hessians (empty arrays).\n",
    "# shape = (n_samples, n_trees_per_iteration).\n",
    "gradient, hessian = clf._loss.init_gradient_and_hessian(\n",
    "    n_samples=n_samples, dtype=G_H_DTYPE, order=\"F\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update gradients and hessians, inplace\n",
    "# Note that clf._loss expects shape (n_samples,) for\n",
    "# n_trees_per_iteration = 1 else shape (n_samples, n_trees_per_iteration).\n",
    "if clf._loss.constant_hessian:\n",
    "    clf._loss.gradient(\n",
    "        y_true=y_train,\n",
    "        raw_prediction=raw_predictions,\n",
    "        sample_weight=sample_weight_train,\n",
    "        gradient_out=gradient,\n",
    "        n_threads=n_threads,\n",
    "    )\n",
    "else:\n",
    "    clf._loss.gradient_hessian(\n",
    "        y_true=y_train,\n",
    "        raw_prediction=raw_predictions,\n",
    "        sample_weight=sample_weight_train,\n",
    "        gradient_out=gradient,\n",
    "        hessian_out=hessian,\n",
    "        n_threads=n_threads,\n",
    "    )\n",
    "\n",
    "# Append a list since there may be more than 1 predictor per iter\n",
    "predictors.append([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2-d views of shape (n_samples, n_trees_per_iteration_) or (n_samples, 1)\n",
    "# on gradient and hessian to simplify the loop over n_trees_per_iteration_.\n",
    "if gradient.ndim == 1:\n",
    "    g_view = gradient.reshape((-1, 1))\n",
    "    h_view = hessian.reshape((-1, 1))\n",
    "else:\n",
    "    g_view = gradient\n",
    "    h_view = hessian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.497],\n",
       "       [ 0.497],\n",
       "       [-0.503],\n",
       "       [ 0.497],\n",
       "       [-0.503],\n",
       "       [-0.503],\n",
       "       [ 0.497],\n",
       "       [ 0.497],\n",
       "       [ 0.497],\n",
       "       [-0.503]], dtype=float32)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g_view[:10,]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.249991],\n",
       "       [0.249991],\n",
       "       [0.249991],\n",
       "       [0.249991],\n",
       "       [0.249991],\n",
       "       [0.249991],\n",
       "       [0.249991],\n",
       "       [0.249991],\n",
       "       [0.249991],\n",
       "       [0.249991]], dtype=float32)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h_view[:10,]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.503],\n",
       "       [ 0.497],\n",
       "       [ 0.497],\n",
       "       [-0.503],\n",
       "       [ 0.497],\n",
       "       [-0.503],\n",
       "       [-0.503],\n",
       "       [ 0.497],\n",
       "       [-0.503],\n",
       "       [ 0.497]], dtype=float32)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g_view[-10:,]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 0\n",
    "\n",
    "grower = TreeGrower(\n",
    "    X_binned=X_binned_train,\n",
    "    gradients=g_view[:, k],\n",
    "    hessians=h_view[:, k],\n",
    "    n_bins=n_bins,\n",
    "    n_bins_non_missing=clf._bin_mapper.n_bins_non_missing_,\n",
    "    has_missing_values=has_missing_values,\n",
    "    is_categorical=clf.is_categorical_,\n",
    "    monotonic_cst=clf.monotonic_cst,\n",
    "    max_leaf_nodes=clf.max_leaf_nodes,\n",
    "    max_depth=clf.max_depth,\n",
    "    min_samples_leaf=clf.min_samples_leaf,\n",
    "    l2_regularization=clf.l2_regularization,\n",
    "    shrinkage=clf.learning_rate,\n",
    "    n_threads=n_threads,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([(-0.0480001 , 3.999856, 16), (-0.0480001 , 3.999856, 16),\n",
       "       ( 1.95199996, 3.999856, 16), (-1.54500014, 3.749865, 15),\n",
       "       (-1.04800013, 3.999856, 16), ( 0.95199993, 3.999856, 16),\n",
       "       ( 3.45500001, 3.749865, 15), (-2.04800016, 3.999856, 16),\n",
       "       ( 1.95199996, 3.999856, 16), (-3.5450002 , 3.749865, 15)],\n",
       "      dtype=[('sum_gradients', '<f8'), ('sum_hessians', '<f8'), ('count', '<u4')])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.asarray(grower.root.histograms)[0][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'depth': 0,\n",
       " 'sample_indices': <MemoryView of 'ndarray' at 0x1283036c0>,\n",
       " 'n_samples': 4000,\n",
       " 'sum_gradients': -2.396106719970703e-05,\n",
       " 'sum_hessians': 999.9639987945557,\n",
       " 'value': 0,\n",
       " 'is_leaf': False,\n",
       " 'children_lower_bound': -inf,\n",
       " 'children_upper_bound': inf,\n",
       " 'partition_start': 0,\n",
       " 'partition_stop': 4000,\n",
       " 'histograms': <MemoryView of 'ndarray' at 0x128303ba0>,\n",
       " 'split_info': <sklearn.ensemble._hist_gradient_boosting.splitting.SplitInfo at 0x1273c73d0>}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grower.root.__dict__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'gain': 764.4530248414894,\n",
       " 'feature_idx': 4,\n",
       " 'bin_idx': 195,\n",
       " 'missing_go_to_left': 0,\n",
       " 'sum_gradient_left': 368.7779925763607,\n",
       " 'sum_hessian_left': 768.472333073616,\n",
       " 'sum_gradient_right': -368.7780165374279,\n",
       " 'sum_hessian_right': 231.49166572093964,\n",
       " 'n_samples_left': 3074,\n",
       " 'n_samples_right': 926,\n",
       " 'value_left': -0.479884540672245,\n",
       " 'value_right': 1.59305094370864,\n",
       " 'is_categorical': 0,\n",
       " 'left_cat_bitset': None}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grower.root.split_info.__dict__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build child nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from heapq import heappush, heappop\n",
    "import numpy as np\n",
    "from timeit import default_timer as time\n",
    "import numbers\n",
    "\n",
    "\n",
    "node = heappop(grower.splittable_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    sample_indices_left,\n",
    "    sample_indices_right,\n",
    "    right_child_pos,\n",
    ") = grower.splitter.split_indices(node.split_info, node.sample_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "depth = node.depth + 1\n",
    "n_leaf_nodes = len(grower.finalized_leaves) + len(grower.splittable_nodes)\n",
    "n_leaf_nodes += 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble._hist_gradient_boosting.grower import TreeNode\n",
    "\n",
    "\n",
    "left_child_node = TreeNode(\n",
    "    depth,\n",
    "    sample_indices_left,\n",
    "    node.split_info.sum_gradient_left,\n",
    "    node.split_info.sum_hessian_left,\n",
    "    value=node.split_info.value_left,\n",
    ")\n",
    "right_child_node = TreeNode(\n",
    "    depth,\n",
    "    sample_indices_right,\n",
    "    node.split_info.sum_gradient_right,\n",
    "    node.split_info.sum_hessian_right,\n",
    "    value=node.split_info.value_right,\n",
    ")\n",
    "\n",
    "node.right_child = right_child_node\n",
    "node.left_child = left_child_node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set start and stop indices\n",
    "left_child_node.partition_start = node.partition_start\n",
    "left_child_node.partition_stop = node.partition_start + right_child_pos\n",
    "right_child_node.partition_start = left_child_node.partition_stop\n",
    "right_child_node.partition_stop = node.partition_stop\n",
    "\n",
    "if not grower.has_missing_values[node.split_info.feature_idx]:\n",
    "    # If no missing values are encountered at fit time, then samples\n",
    "    # with missing values during predict() will go to whichever child\n",
    "    # has the most samples.\n",
    "    node.split_info.missing_go_to_left = (\n",
    "        left_child_node.n_samples > right_child_node.n_samples\n",
    "    )\n",
    "\n",
    "grower.n_nodes += 2\n",
    "grower.n_categorical_splits += node.split_info.is_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grower.max_depth is not None and depth == grower.max_depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if grower.max_leaf_nodes is not None and n_leaf_nodes == grower.max_leaf_nodes:\n",
    "#     grower._finalize_leaf(left_child_node)\n",
    "#     grower._finalize_leaf(right_child_node)\n",
    "#     grower._finalize_splittable_nodes()\n",
    "#     # return left_child_node, right_child_node\n",
    "\n",
    "if grower.max_depth is not None and depth == grower.max_depth:\n",
    "    grower._finalize_leaf(left_child_node)\n",
    "    grower._finalize_leaf(right_child_node)\n",
    "    # return left_child_node, right_child_node\n",
    "else:\n",
    "    # We will compute the histograms of both nodes even if one of them\n",
    "    # is a leaf, since computing the second histogram is very cheap\n",
    "    # (using histogram subtraction).\n",
    "    n_samples_left = left_child_node.sample_indices.shape[0]\n",
    "    n_samples_right = right_child_node.sample_indices.shape[0]\n",
    "    if n_samples_left < n_samples_right:\n",
    "        smallest_child = left_child_node\n",
    "        largest_child = right_child_node\n",
    "    else:\n",
    "        smallest_child = right_child_node\n",
    "        largest_child = left_child_node\n",
    "\n",
    "    smallest_child.histograms = grower.histogram_builder.compute_histograms_brute(\n",
    "        smallest_child.sample_indices\n",
    "    )\n",
    "    largest_child.histograms = (\n",
    "        grower.histogram_builder.compute_histograms_subtraction(\n",
    "            node.histograms, smallest_child.histograms\n",
    "        )\n",
    "    )\n",
    "    grower._compute_best_split_and_push(left_child_node)\n",
    "    grower._compute_best_split_and_push(right_child_node)\n",
    "\n",
    "    # # For LEAF nodes, release memory used by histograms as they are no longer needed\n",
    "    # # since they won't be split.\n",
    "    # for child in (left_child_node, right_child_node):\n",
    "    #     if child.is_leaf:\n",
    "    #         del child.histograms\n",
    "    \n",
    "    # # For INTERNAL nodes, release memory used by histograms as they are no longer needed\n",
    "    # # once children histograms have been computed.\n",
    "    # del node.histograms\n",
    "\n",
    "# if left_child_node.n_samples < grower.min_samples_leaf * 2:\n",
    "#     grower._finalize_leaf(left_child_node)\n",
    "# if right_child_node.n_samples < grower.min_samples_leaf * 2:\n",
    "#     grower._finalize_leaf(right_child_node)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grower.splittable_nodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make predictors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>value</th>\n",
       "      <th>count</th>\n",
       "      <th>feature_idx</th>\n",
       "      <th>num_threshold</th>\n",
       "      <th>missing_go_to_left</th>\n",
       "      <th>left</th>\n",
       "      <th>right</th>\n",
       "      <th>gain</th>\n",
       "      <th>depth</th>\n",
       "      <th>is_leaf</th>\n",
       "      <th>bin_threshold</th>\n",
       "      <th>is_categorical</th>\n",
       "      <th>bitset_idx</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>4000</td>\n",
       "      <td>4</td>\n",
       "      <td>0.394379</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>764.453025</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>195</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.479885</td>\n",
       "      <td>3074</td>\n",
       "      <td>1</td>\n",
       "      <td>0.154310</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>775.897224</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>165</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-1.217273</td>\n",
       "      <td>1998</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.889355</td>\n",
       "      <td>1076</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.593051</td>\n",
       "      <td>926</td>\n",
       "      <td>3</td>\n",
       "      <td>1.029580</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>43.621109</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>216</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1.720982</td>\n",
       "      <td>852</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.120112</td>\n",
       "      <td>74</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      value  count  feature_idx  num_threshold  missing_go_to_left  left  \\\n",
       "0  0.000000   4000            4       0.394379                   1     1   \n",
       "1 -0.479885   3074            1       0.154310                   1     2   \n",
       "2 -1.217273   1998            0       0.000000                   0     0   \n",
       "3  0.889355   1076            0       0.000000                   0     0   \n",
       "4  1.593051    926            3       1.029580                   1     5   \n",
       "5  1.720982    852            0       0.000000                   0     0   \n",
       "6  0.120112     74            0       0.000000                   0     0   \n",
       "\n",
       "   right        gain  depth  is_leaf  bin_threshold  is_categorical  \\\n",
       "0      4  764.453025      0        0            195               0   \n",
       "1      3  775.897224      1        0            165               0   \n",
       "2      0   -1.000000      2        1              0               0   \n",
       "3      0   -1.000000      2        1              0               0   \n",
       "4      6   43.621109      1        0            216               0   \n",
       "5      0   -1.000000      2        1              0               0   \n",
       "6      0   -1.000000      2        1              0               0   \n",
       "\n",
       "   bitset_idx  \n",
       "0           0  \n",
       "1           0  \n",
       "2           0  \n",
       "3           0  \n",
       "4           0  \n",
       "5           0  \n",
       "6           0  "
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "pd.DataFrame(grower.make_predictor(clf._bin_mapper.bin_thresholds_).nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1.94625154, -0.9431073 , -0.34084117, ..., -0.75749854,\n",
       "        2.67137161, -2.96930792])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([   4.,   16.,   85.,  253.,  756., 1102.,  937.,  596.,  219.,\n",
       "          32.]),\n",
       " array([-7.05675295, -5.92919655, -4.80164015, -3.67408376, -2.54652736,\n",
       "        -1.41897096, -0.29141457,  0.83614183,  1.96369822,  3.09125462,\n",
       "         4.21881102]),\n",
       " <BarContainer object of 10 artists>)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(X_train[:, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2604"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(X_train[:, 1] <= 0.154)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2446"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(X_train[:, 1] <= -0.027)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "da3de42dcda9fd7691867a6931ab9341a8446acb0162577900f5e63b41b9f416"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 ('scikit-learn-bjglRoMY')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
